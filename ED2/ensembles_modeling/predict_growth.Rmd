---
title: "Predicting Setaria growth by modeling ensemble results"
author: Kristina Riemer
output: github_document
---

## Purpose

Predict is to predict Setaria growth for modified plant (e.g., anthocyanin) at new
sites and times. Inputs that can be changed are parameter values and site-level
environmental variables. 

Necessary packages: 
```{r}
library(PEcAn.settings)
library(dplyr)
library(tidyr)
library(lubridate)
library(daymetr)
library(ggplot2)
library(tidymodels)
```


## Data

Combine ED2 output variables from ensemble runs with parameter values and environmental
data by month for all sites with ED2 results. 

Steps: 

1. Get cumulative monthly NPP by ensemble and site (from ensemble.ts .Rdata file, which has hourly values by ensemble)
2. Get parameter values for each ensemble run (from ensemble.samples .Rdata file; same across years)
3. Get monthly environmental values for each site and year (lookup from MERRA or another gridded resource)

Five sites: 

```{r}
sites <- c()
site_list_kr <- c("LW", "SR", "WB")
for(site in site_list_kr){
  pecan_xml_path <- paste0("/data/tests/ed2_transect_", site, "/pecan.CHECKED.xml")
  pecan_xml <- read.settings(pecan_xml_path)
  site <- data.frame(name = pecan_xml$run$site$name, id = pecan_xml$run$site$id,
                     lat = pecan_xml$run$site$lat, lon = pecan_xml$run$site$lon, 
                     start = pecan_xml$run$start.date, end = pecan_xml$run$end.date, 
                     site = site)
  sites <- bind_rows(sites, site)
}

pecan_xml_NC <- read.settings("/data/output/pecan_runs/transect_runs/ed2_transect_NC/pecan_checked_2022-08-12.xml")
site_NC <- data.frame(name = "North Carolina Loblolly Pine (US-NC2)", id = pecan_xml_NC$run$site$id,
                     lat = pecan_xml_NC$run$site$lat, lon = pecan_xml_NC$run$site$lon, 
                     start = pecan_xml_NC$run$start.date, end = pecan_xml_NC$run$end.date, 
                     site = "NC")
sites <- bind_rows(sites, site_NC)

pecan_xml_WL <- read.settings("/data/output/pecan_runs/transect_runs/ed2_transect_WL/pecan_checked.xml")
site_WL <- data.frame(name = pecan_xml_WL$run$site$name, id = pecan_xml_WL$run$site$id,
                     lat = pecan_xml_WL$run$site$lat, lon = pecan_xml_WL$run$site$lon, 
                     start = pecan_xml_NC$run$start.date, end = pecan_xml_NC$run$end.date, 
                     site = "WL") 
sites <- bind_rows(sites, site_WL) %>% 
  mutate(lat = as.numeric(lat), 
         lon = as.numeric(lon))

```
```{r}
library(leaflet)
leaflet(sites) %>% addTiles() %>%
  addCircleMarkers(lng = ~lon, lat = ~lat, 
             popup = ~name)
```

### Step 1: ensemble results data

Code from [`plot.R`](https://github.com/cct-datascience/model-vignettes/blob/6a77020a1218334cc8243f3f4b038fb99c947006/ED2/SR_recent_mult_set_run/plot.R)

Get this by PFT, for Setaria
To limit to just Setaria PFT, need to pull directly from ED2 h5 files, use npp_out csv file
TODO: generate npp_out for three previous sites
Just August of first year, to start
This comes monthly? 

```{r}
site_list_ec <- c("NC", "WL")
ens_results <- c()
for(site in site_list_ec){
  ens_path <- paste0("/data/output/pecan_runs/transect_runs/ed2_transect_", site, "/npp_out.csv")
  single_site_ens_results <- read.csv(ens_path) %>% 
    filter(pft == 1, 
           grepl("-08-", date)) %>% 
    select(ensemble, npp, date) %>% 
    mutate(ensemble = stringr::str_remove(substr(ensemble, 8, 9), "^0+"), 
           site = site)
  ens_results <- bind_rows(single_site_ens_results, ens_results)
}

for(site in site_list_kr){
  ens_path <- paste0("/data/tests/ed2_transect_", site, "/npp_out.csv")
  single_site_ens_results <- read.csv(ens_path) %>% 
    filter(pft == 1, 
           grepl("-08-", date)) %>% 
    select(ensemble, npp, date) %>% 
    mutate(ensemble = stringr::str_remove(substr(ensemble, 8, 9), "^0+"), 
           site = site)
  ens_results <- bind_rows(single_site_ens_results, ens_results)
}

ens_results <- ens_results%>% 
  mutate(year = year(ymd(date))) %>% 
  select(-date)

```

### Step 2: parameter values

For only Setaria PFT? 
Assuming order of ensembles is the same in this file as in the ensembles result

```{r}
params <- c()
for(site in site_list_ec){
  param_path <- paste0("/data/output/pecan_runs/transect_runs/ed2_transect_", site, "/ensemble.samples.NOENSEMBLEID.Rdata")
  load(param_path)

  single_site_params <- ens.samples$SetariaWT %>% 
    tibble::rownames_to_column() %>% 
    mutate(ensemble = rowname) %>% 
    select(-rowname) %>% 
    mutate(site = site)
  params <- bind_rows(single_site_params, params)
  
  rm(ens.samples)
}

for(site in site_list_kr){
    param_path <- paste0("/data/tests/ed2_transect_", site, "/ensemble.samples.NOENSEMBLEID.Rdata")
  load(param_path)

  single_site_params <- ens.samples$SetariaWT %>% 
    tibble::rownames_to_column() %>% 
    mutate(ensemble = rowname) %>% 
    select(-rowname) %>% 
    mutate(site = site)
  params <- bind_rows(single_site_params, params)
  
  rm(ens.samples)
}

```

Combine parameters with ensemble results by site and ensemble. 

```{r}
ens_params <- left_join(ens_results, params, by = c("ensemble", "site"))
```

### Step 3: environmental data


```{r cache=TRUE}
mets <- list()
for(i in 1:nrow(sites)){
    site <- sites[i,]
    tmp <- 
        download_daymet(
                    site = site$site,
                    lat = site$lat, 
                    lon = site$lon, 
                    start = year(ymd(site$start)),
                    end = 2019 #year(ymd(site$end))
                    )
    mets[[site$name]] <- cbind(site = tmp$site,
                               lat = tmp$lat,
                               lon = tmp$longitude,
                               alt = tmp$altitude,
                               tmp$data)
}

mets_all <- dplyr::bind_rows(mets)
readr::write_csv(mets_all, 'mets_all.csv')

mymean <- function(x) {
    a <- mean(x, na.rm = TRUE)
    b <- signif(a, 4)
    return(b)
}

mets_mean_summer <- mets_all  %>% 
#  group_by(site) %>% 
#  filter(year == min(year)) %>% 
  mutate(date = as.Date(yday - 1, origin = paste0(year, "-01-01")), 
         month = month(ymd(date))) %>% 
  #filter(month %in% c(7, 8)) %>% 
  group_by(site, year) %>% 
  summarise(
    mean_temp = mymean((tmax..deg.c. + tmin..deg.c.)/2),
    mean_vpd = mymean(vp..Pa.),
    mean_precip = mymean(prcp..mm.day.),
    mean_srad = mymean(srad..W.m.2.),
    mean_swe = mymean(swe..kg.m.2.),
    mean_dayl = mymean(dayl..s.)/86400)
```

```{r}
ens_params_env <- left_join(ens_params, mets_mean_summer, by = c("site", "year")) %>%
  janitor::clean_names()
```


## Model

Machine learning model for prediction? 

Need to get new environmental data given a site and time

Goal: predict NPP given parameter and environmental input, so for modified Setaria at new site

supervised learning, regression
features (=variables, or columns in data) should be independent and informative (remove some params and env? are measured params more informative?)
linear relationships? 
which of these are photosynthetic params? 
```{r}
ggplot(ens_params_env, aes(x = quantum_efficiency, y = npp)) +
  geom_point() + 
  facet_grid(vars(site))

ggplot(ens_params_env, aes(x = mean_temp, y = npp)) +
  geom_point() + 
  facet_grid(vars(site))
ggplot(ens_params_env, aes(x = mean_precip, y = npp)) +
  geom_point() + 
  facet_grid(vars(site))
```

### Simple models

```{r}
aov(npp ~ ., data = ens_params_env) %>% 
  broom::tidy() %>% 
  mutate_if(is.numeric, signif, 3)

z <- aov(npp ~ ., data = ens_params_env %>% select(-ensemble, -site, -year, -mean_dayl)) 
zz <- z %>% 
  broom::tidy() %>% 
  mutate_if(is.numeric, signif, 3)
zz
zz %>% # print out names of variables to keep
  filter(p.value < 0.1) %>% 
  select(term) %>% 
  gsub('\"', '', .) %>% 
  paste0(collapse = ', ')
options(scipen=2)
lm(npp ~ ., data = ens_params_env) %>% 
  broom::tidy() %>% 
  mutate_if(is.numeric, signif, 3)

```

only sensible if we remove year, ensemble, site

### Advanced models


naive bayes? -nope, for classification
decision tree or random forest? (latter is more accurate than interpretable)
accuracy vs interpretability? former

for predicting, give it the same input data but change the quantum efficiency parameter? 



### subset dataset

```{r}
set.seed(1234)
ens_params_env %>% group_by(site, year) %>% summarise(n = n())

t <- ens_params_env %>% 
  filter(site == "WL" & year > 2012) %>% 
  select(-site, -year, -ensemble, # these are accounted for by other variables
         -mean_dayl) # these aren't of primary interest
data_split <- initial_split(ens_params_env)
data_train <- t #%>% #training(data_split) 
                #train on entire thing. can always generate new test data 
  # select(-ensemble, -site, -year)
  #select(npp, mort2, growth_resp_factor, leaf_turnover_rate, leaf_width, fineroot2leaf, seedling_mortality, mean_vpd, mean_precip, mean_srad)
  # select what is significant from aov above
  # 

#data_test <- testing(data_split)

```
## Random forest

preprocess training data
```{r}
data_recipe <- recipe(npp ~ ., data = data_train) #%>% 
  #step_naomit()
  # step_rm(c(ensemble, site, year)) 
  # update_role(ensemble, new_role = "ID") %>% 
  # update_role(site, new_role = "ID")
#step_rm to remove predictors
```


### Linear Regression

```{r}
lm_model <- 
  linear_reg() %>% 
  set_engine("lm")

lm_wf <- workflow() %>% 
  add_recipe(data_recipe) %>% 
  add_model(lm_model)

model_fit <- lm_wf %>% 
  fit(data = data_train)

folds <- vfold_cv(data_train)
resamp_wf <- workflow() %>% 
  add_recipe(data_recipe) %>% 
  add_model(lm_model)
resamp_fit <- fit_resamples(resamp_wf, folds, 
                            #metrics = metric_set(accuracy, sens, spec),
                            control = control_resamples(save_pred = TRUE))
resamp_metrics <- collect_metrics(resamp_fit)

resamp_metrics
```

### Random Forest
specify model
```{r}
rf_model <- rand_forest() %>% 
  set_mode("regression") %>% 
  set_engine("ranger", importance = 'impurity')
```

do workflow
```{r}
data_wf <- workflow() %>% 
  add_recipe(data_recipe) %>% 
  add_model(rf_model)
```

fit model
```{r}
model_fit <- data_wf %>% 
  fit(data = data_train)
```

cross-validate data and evaluate
```{r}
folds <- vfold_cv(data_train)
resamp_wf <- workflow() %>% 
  add_recipe(data_recipe) %>% 
  add_model(rf_model)
resamp_fit <- fit_resamples(resamp_wf, folds, 
                            #metrics = metric_set(accuracy, sens, spec),
                            control = control_resamples(save_pred = TRUE))
resamp_metrics <- collect_metrics(resamp_fit)
resamp_metrics
```

rmse: average deviation between predicted npp and measured npp; 0 means perfect fit, closer to zero is better
rsq; how much variation in npp explained by predictor variables; from 0 to 1, closer to 1 is better

```{r}
rmse <- resamp_metrics %>% filter(.metric == 'rmse') %>% select(mean) 
rmse/mean(data_train$npp) #this is high, want it below 10%? 
```


options to improve model: 

- reduce features?
- hypertune parameters


### Save environment

```{r}
save.image('~/model-vignettes/ED2/ensembles_modeling/predict_growth.Rdata')
```

## Predictions


#### Step 1. Create a grid of met data

```{r, eval = false}

newlats <- seq(35, 40, by = 0.25)
newlons <- seq(-105, -80, by = 0.25)
n <- expand.grid(newlats, newlons)
dim(n)
newsites <- data.frame(site = 1:nrow(n), n)
colnames(newsites) <- c('site', 'lat', 'lon')
sitesfile <- paste0(tempdir(),"/sites.csv")
write.table(newsites, sitesfile,
           sep = ",",
           col.names = TRUE,
           row.names = FALSE,
           quote = FALSE)
ptm <- proc.time()
df_batch <- download_daymet_batch(
  file_location = sitesfile,
  start = 2010,
  end = 2010,
  internal = TRUE)
proc.time() - ptm

newmets <- list()
for(i in 1:length(df_batch)){
  tmp <- df_batch[[i]]
  newmets[[i]] <- cbind(site = tmp$site,
                        lat = tmp$lat,
                        lon = tmp$longitude,
                        alt = tmp$altitude,
                        tmp$data)
}

```

```{r}
#getwd()
#file.copy('../../newmets.Rdata', './newmets.Rdata')
load('./newmets.Rdata')
```


```{r}
newmets_all <- dplyr::bind_rows(newmets)
#readr::write_csv(mets_all, 'mets_all.csv')

mymean <- function(x) {
    a <- mean(x, na.rm = TRUE)
    b <- signif(a, 4)
    return(b)
}

newmets_mean_summer <- newmets_all  %>% 
#  group_by(site) %>% 
#  filter(year == min(year)) %>% 
  mutate(date = as.Date(yday - 1, origin = paste0(year, "-01-01")), 
         month = month(ymd(date))) %>% 
  #filter(month %in% c(7, 8)) %>% 
  group_by(site, year) %>% 
  summarise(
    mean_temp = mymean((tmax..deg.c. + tmin..deg.c.)/2),
    mean_vpd = mymean(vp..Pa.),
    mean_precip = mymean(prcp..mm.day.),
    mean_srad = mymean(srad..W.m.2.),
    mean_swe = mymean(swe..kg.m.2.),
    mean_dayl = mymean(dayl..s.)/86400)
```

```{r}
sa <- sa.samples$SetariaWT
example_wt_data 
newpreds <- cbind(example_wt_data,
                  newmets_mean_summer) %>% 
  left_join(newsites %>% mutate(site = as.character(site)), by = 'site')
```


### Step 2 create parameterizations for mutants


#### 2.1 WT

Use model predictions to get wild type npp using median values from sensitivity analysis

```{r}

load('/data/tests/ed2_SR_recent_sa/sensitivity.samples.NOENSEMBLEID.Rdata')
example_wt_data <- as.data.frame(sa.samples$SetariaWT['50',]) %>%
  janitor::clean_names()

```


#### 2.2 Mutants

change by 25% or +/- 1 SD

- fineroot2leaf, would be higher with shorter plant
- quantum_efficiency, would be lower with antho plant
- stomatal_slope & cuticular conductance, would be higher with cooler leaf


```{r}
newpreds_lowtemp <- newpreds %>% 
  mutate(stomatal_slope = 0.75 * stomatal_slope,#sa$stomatal_slope[1],
         cuticular_cond = 0.75 * cuticular_cond)#sa$cuticular_cond[1])

newpreds_antho <- newpreds %>% 
  mutate(quantum_efficiency = 0.75 * quantum_efficiency)

newpreds_short <- newpreds %>% 
  mutate(fineroot2leaf = 1.25 * fineroot2leaf)#sa$fineroot2leaf[1])


npp_pred <- predict(model_fit, 
                    newpreds, 
                    type = "numeric")$.pred
npp_pred_lowtemp <- predict(model_fit, 
                            newpreds_lowtemp, 
                            type = "numeric")$.pred

npp_pred_antho <- predict(model_fit, 
                            newpreds_antho, 
                            type = "numeric")$.pred
npp_pred_short <- predict(model_fit, 
                            newpreds_short, 
                            type = "numeric")$.pred

preds <- data.frame(lat     = newpreds$lat,
                    lon     = newpreds$lon,
                    wt      = npp_pred,
                    lowtemp = npp_pred_lowtemp,
                    anthox  = npp_pred_antho,
                    short   = npp_pred_short)

dpreds <- preds %>% 
  transmute(lat = lat,
            lon = lon,
            d_lowtemp = (wt - lowtemp)/wt,
            d_anthox  = (wt - anthox)/wt,
            d_short   = (wt - short)/wt)
```


```{r}
predslong <- preds %>% 
  pivot_longer(cols = c(-lat, -lon))
library(ggthemes)
library(ggmap)
library(usmap)
ggplot(data = predslong) + 
  geom_tile(aes(x = lon, y = lat, fill = value)) + 
  theme_map() +
  facet_wrap(~name)

```

```{r}
NA_background <- map_data("state")

p <- ggplot() +
  geom_polygon(data = NA_background, 
               aes(x = long, y = lat, group = group), 
               fill = "white", color = "black") +
  geom_raster(data = predslong, aes(x = lon, y = lat, fill = value), 
              alpha = 0.9) +
#  scale_fill_gradient2(low = "maroon", mid = "lightblue", high = "darkblue", 
#                       midpoint = 1, 
#                       breaks = c(0.3, 1, 1.2), 
#                       limits = c(0.3, 1.2)) +
  coord_cartesian(xlim = c(-108, -78), ylim = c(33, 42)) +
  theme_classic(base_size = 12) +
  theme(panel.background = element_rect(fill = "grey", colour = "grey"), 
        panel.grid.major = element_line(colour = "grey"),
        panel.grid.minor = element_line(colour = "grey")) +
  labs(x = "", y = "", fill = "") +
  facet_wrap(~name)
#  transition_manual(doy) +
#  ggtitle('Day: {current_frame}')
```

```{p}
library(ggnewscale) # free color scales for faceted plot

library('RColorBrewer')
dpredslong <- dpreds %>% 
  pivot_longer(cols = c(-lat, -lon))


dp <- list()  
color_scales <- c("YlOrRd", "YlOrBr", "YlGnBu")

for(i in 1:length(unique(dpredslong$name))){
dname <- unique(dpredslong$name)[i]
df <- dpredslong %>% 
  filter(name == dname) %>% 
  mutate(value = 100 * value)
dp[[dname]] <- ggplot() +
  geom_polygon(data = NA_background, 
               aes(x = long, y = lat, group = group), 
               fill = "white", color = "black") +
  
  geom_raster(data = df, aes(x = lon, y = lat, fill = value), 
              alpha = 0.9) +
  # scale_fill_gradient2(low = "maroon", mid = "lightblue", high = "darkblue", 
  #                      midpoint = 0, 
  #                      breaks = c(-0.1, 0, 0.1), 
  #                      limits = c(-0.1, 0.1)) +
  scale_fill_brewer(
    palette = color_scales[i]) +
  coord_cartesian(xlim = c(-108, -78), ylim = c(33, 42))  +
  theme_classic(base_size = 12) +
  theme(panel.background = element_rect(fill = "grey", colour = "grey"), 
        panel.grid.major = element_line(colour = "grey"),
        panel.grid.minor = element_line(colour = "grey")) +
  
  labs(x = "", y = "", fill = "") +
  facet_wrap(~name)
}

cowplot::plot_grid(plotlist = dp)
```


```{r}
library(vip)
library(tidymodels)
library(ranger)
library(dplyr)

labels <- c(
  mort2 = 'Mortality', 
  growth_resp_factor = 'Growth Respiration', 
  leaf_turnover_rate = 'Leaf Turnover Rate', 
  leaf_width = 'Leaf Width', 
  nonlocal_dispersal = 'Seed Dispersal', 
  fineroot2leaf = 'Root:Leaf C allocation', 
  root_turnover_rate = "Root Turnover Rate", 
  seedling_mortality = "Seedling Mortality", 
  stomatal_slope = "Stomatal Slope", 
  quantum_efficiency = "Quantum Efficiency", 
  vcmax = "Vcmax", 
  r_fract = "Respiration Fraction", 
  cuticular_cond = "Cuticular Conductance", 
  root_respiration_rate = "Root Respiration", 
  vm_low_temp = "Min Temp Photosynthesis", 
  sla = "Specific Leaf Area", 
  mean_temp = "Air Temperature", 
  mean_vpd = "Vapor Pressure Deficit", 
  mean_precip = "Precipitation", 
  mean_srad = "Solar Radiation", 
  mean_swe = "Soil Moisture"
)
model_fit%>% 
  extract_fit_parsnip()

# requires tidymodels v1.0
p <- model_fit%>% 
  extract_fit_parsnip() %>%
  vip(num_features = 10, geom = 'point') 

pp <- p + theme_minimal() + scale_x_discrete(labels = labels) + scale_y_continuous(limits = c(80, 180))

pp$layers[[1]]$aes_params$size <- 2

  

```